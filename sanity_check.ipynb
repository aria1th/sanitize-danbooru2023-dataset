{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Locate danbooru2023.db and db.py in the same directory as this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from db import *\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "\n",
    "log_file = \"danbooru.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "class CachedRequest:\n",
    "    \"\"\"\n",
    "    Wrapper for requests to cache get method\n",
    "    This is for avoiding rate limiting\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_file=\"cache.jsonl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if os.path.isfile(self.cache_file):\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        self.cache[data[\"url\"]] = data[\"response\"]\n",
    "                    except Exception as e:\n",
    "                        logging.exception(\"Error loading cache: {}, skipping line\".format(e))\n",
    "    def get(self, url):\n",
    "        if url in self.cache:\n",
    "            return self.cache[url]\n",
    "        else:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()\n",
    "            to_json = {\"url\": url, \"response\": r.json()}\n",
    "            self.cache[url] = to_json[\"response\"]\n",
    "            with open(self.cache_file, \"a\") as f:\n",
    "                f.write(json.dumps(to_json) + \"\\n\")\n",
    "            return to_json[\"response\"]\n",
    "\n",
    "class DifferenceCache:\n",
    "    \"\"\"\n",
    "    Wrapper for caching differences\n",
    "    If calculated difference exists, we will use it instead of calculating it again\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_file=\"difference_cache.jsonl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if os.path.isfile(self.cache_file):\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        self.cache[data[\"id\"]] = data[\"difference\"]\n",
    "                    except Exception as e:\n",
    "                        logging.exception(\"Error loading cache: {}, skipping line\".format(e))\n",
    "    def get(self, post_id):\n",
    "        # if tuple, unpack\n",
    "        if isinstance(post_id, tuple):\n",
    "            assert len(post_id) == 1, \"post_id tuple must be of length 1\"\n",
    "            post_id = post_id[0]\n",
    "        if post_id in self.cache:\n",
    "            return self.cache[post_id]\n",
    "        else:\n",
    "            difference = compare_info(post_id)\n",
    "            to_json = {\"id\": post_id, \"difference\": difference}\n",
    "            self.cache[post_id] = to_json[\"difference\"]\n",
    "            with open(self.cache_file, \"a\") as f:\n",
    "                f.write(json.dumps(to_json) + \"\\n\")\n",
    "            return to_json[\"difference\"]\n",
    "    def contains(self, post_id):\n",
    "        return post_id in self.cache\n",
    "    \n",
    "class PostPatchStateCache:\n",
    "    \"\"\"\n",
    "    Wrapper for caching post patch states\n",
    "    Returns True if post is patched, False if not patched\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_file=\"post_patch_state_cache.jsonl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if os.path.isfile(self.cache_file):\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        self.cache[data[\"id\"]] = data[\"state\"]\n",
    "                    except Exception as e:\n",
    "                        logging.exception(\"Error loading cache: {}, skipping line\".format(e))\n",
    "    def get(self, post_id):\n",
    "        return post_id in self.cache\n",
    "    \n",
    "    def set(self, post_id, state:bool=True):\n",
    "        self.cache[post_id] = state\n",
    "        to_json = {\"id\": post_id, \"state\": state}\n",
    "        with open(self.cache_file, \"a\") as f:\n",
    "            f.write(json.dumps(to_json) + \"\\n\")\n",
    "        return to_json[\"state\"]\n",
    "\n",
    "class TagCreationCache:\n",
    "    \"\"\"\n",
    "    Wrapper for caching tag creation\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_file=\"tag_creation_cache.jsonl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if os.path.isfile(self.cache_file):\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        self.cache[data[\"id\"]] = {\"tag_name\": data[\"tag_id\"], \"tag_context\": data[\"tag_name\"]}\n",
    "                    except Exception as e:\n",
    "                        logging.exception(\"Error loading cache: {}, skipping line\".format(e))\n",
    "    def init_tags(self):\n",
    "        \"\"\"\n",
    "        Initialize tags\n",
    "        \"\"\"\n",
    "        for tag_id in self.cache:\n",
    "            tag = Tag.get_or_none(Tag.id == tag_id)\n",
    "            if tag is None:\n",
    "                tag = Tag.create(id=tag_id, name=self.cache[tag_id][\"tag_name\"], type=self.cache[tag_id][\"tag_context\"], popularity=-1)\n",
    "                logging.info(\"Created tag {} with id {}\".format(self.cache[tag_id][\"tag_name\"], tag.id))\n",
    "    \n",
    "    def set(self, tag_id, tag_name, tag_context):\n",
    "        self.cache[tag_id] = {\"tag_name\": tag_name, \"tag_context\": tag_context}\n",
    "        to_json = {\"id\": tag_id, \"tag_name\": tag_name, \"tag_context\": tag_context}\n",
    "        with open(self.cache_file, \"a\") as f:\n",
    "            f.write(json.dumps(to_json) + \"\\n\")\n",
    "        return to_json\n",
    "\n",
    "requests_cache = CachedRequest()\n",
    "rating_dict = {\"s\": \"sensitive\", \"q\": \"questionable\", \"e\": \"explicit\", \"g\": \"general\"}\n",
    "\n",
    "difference_database = DifferenceCache()\n",
    "\n",
    "patched_posts = PostPatchStateCache()\n",
    "\n",
    "tag_creation_cache = TagCreationCache()\n",
    "tag_creation_cache.init_tags()\n",
    "\n",
    "def convert_tag_ids_to_names(tag_ids: Union[int, Tag, List[Union[int, Tag]]]) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert tag ids to tag names\n",
    "    \"\"\"\n",
    "    if isinstance(tag_ids, int):\n",
    "        return Tag.get_by_id(tag_ids).name\n",
    "    elif isinstance(tag_ids, Tag):\n",
    "        return tag_ids.name\n",
    "    elif isinstance(tag_ids, list):\n",
    "        return [convert_tag_ids_to_names(tag_id) for tag_id in tag_ids]\n",
    "    else:\n",
    "        raise TypeError(f\"tag_ids must be int, Tag or List[int, Tag] but got {type(tag_ids)}\")\n",
    "\n",
    "def create_tag(string:str, tag_context=\"general\"):\n",
    "    \"\"\"Create a tag in the database\"\"\"\n",
    "    tag = Tag.get_or_none(Tag.name == string)\n",
    "    if tag is None:\n",
    "        tag = Tag.create(name=string,type=tag_context,popularity=-1)\n",
    "        logging.info(\"Created tag {} with id {}\".format(string,tag.id))\n",
    "        tag_creation_cache.set(tag.id, tag_name=string, tag_context=tag_context)\n",
    "    return tag\n",
    "\n",
    "def convert_string_to_tag_ids(tag_names: Union[str, List[str]], context=\"general\") -> Union[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Convert tag names to tag ids\n",
    "    \"\"\"\n",
    "    if isinstance(tag_names, str):\n",
    "        return create_tag(tag_names,context).id\n",
    "    elif isinstance(tag_names, list):\n",
    "        return [convert_string_to_tag_ids(tag_name) for tag_name in tag_names]\n",
    "    else:\n",
    "        raise TypeError(f\"tag_names must be str or List[str] but got {type(tag_names)}\")\n",
    "\n",
    "def get_id_from_tag(tag:Union[Tag,List[Tag]]):\n",
    "    if isinstance(tag,Tag):\n",
    "        return tag.id\n",
    "    elif isinstance(tag,list):\n",
    "        return [get_id_from_tag(t) for t in tag]\n",
    "    else:\n",
    "        raise TypeError(f\"tag must be Tag or List[Tag] but got {type(tag)}\")\n",
    "\n",
    "\n",
    "def check_danbooru_post(post_id,by_id=False):\n",
    "    url = \"https://danbooru.donmai.us/posts/{}.json\".format(post_id)\n",
    "    r = requests_cache.get(url)\n",
    "    result_dict = {\n",
    "        \"id\" : r[\"id\"],\n",
    "        \"file_url\" : r[\"large_file_url\"] if \"large_file_url\" in r else r.get(\"file_url\",None), # use large_file_url if available (for high res images)\n",
    "        \"rating\" : rating_dict[r[\"rating\"]],\n",
    "        \"year\" : r[\"created_at\"][0:4],\n",
    "        \"score\" : r[\"score\"],\n",
    "        \"fav_count\" : r[\"fav_count\"],\n",
    "        \"tag_list_general\" : r[\"tag_string_general\"].split(\" \"),\n",
    "        \"tag_list_character\" : r[\"tag_string_character\"].split(\" \"),\n",
    "        \"tag_list_artist\" : r[\"tag_string_artist\"].split(\" \"),\n",
    "        \"tag_list_meta\" : r[\"tag_string_meta\"].split(\" \"),\n",
    "        \"tag_list_copyright\" : r[\"tag_string_copyright\"].split(\" \"),\n",
    "    }\n",
    "    if by_id:\n",
    "        for key in result_dict:\n",
    "            if \"tag_list\" not in key:\n",
    "                continue\n",
    "            result_dict[key] = convert_string_to_tag_ids(result_dict[key],key.split(\"_\")[2])\n",
    "    return result_dict\n",
    "\n",
    "def check_database_post(post_id,by_id=True):\n",
    "    post = Post.get_or_none(Post.id == post_id)\n",
    "    if post is None:\n",
    "        return None\n",
    "    else:\n",
    "        result_dict = {\n",
    "            \"id\" : post.id,\n",
    "            \"file_url\" : post.large_file_url if post.large_file_url is not None else getattr(post,\"file_url\",None), # use large_file_url if available (for high res images)\n",
    "            \"rating\" : rating_dict[post.rating],\n",
    "            \"year\" : post.created_at[0:4],\n",
    "            \"score\" : post.score,\n",
    "            \"fav_count\" : post.fav_count,\n",
    "            \"tag_list_general\" : get_id_from_tag(post.tag_list_general),\n",
    "            \"tag_list_character\" : get_id_from_tag(post.tag_list_character),\n",
    "            \"tag_list_artist\" : get_id_from_tag(post.tag_list_artist),\n",
    "            \"tag_list_meta\" : get_id_from_tag(post.tag_list_meta),\n",
    "            \"tag_list_copyright\" : get_id_from_tag(post.tag_list_copyright),\n",
    "        }\n",
    "        if not by_id:\n",
    "            for key in result_dict:\n",
    "                if \"tag_list\" not in key:\n",
    "                    continue\n",
    "                result_dict[key] = convert_tag_ids_to_names(result_dict[key])\n",
    "        return result_dict\n",
    "\n",
    "def compare_info(post_id, by_id=False):\n",
    "    \"\"\"\n",
    "    Compare danbooru and database info\n",
    "    Returns the difference dict, <new info>, <old info>\n",
    "    \"\"\"\n",
    "    difference_dict = {},{}\n",
    "    assert isinstance(post_id, int), f\"post_id must be int but got {type(post_id)} with value {post_id}\"\n",
    "    danbooru_info = check_danbooru_post(post_id,by_id=by_id)\n",
    "    database_info = check_database_post(post_id,by_id=by_id)\n",
    "    if database_info is None:\n",
    "        return None, danbooru_info\n",
    "    for key in danbooru_info:\n",
    "        # check \"tag_list\" keys\n",
    "        if \"tag_list\" in key:\n",
    "            if set(danbooru_info[key]) != set(database_info[key]):\n",
    "                difference_dict[0][key] = set(danbooru_info[key]) - set(database_info[key])\n",
    "                difference_dict[1][key] = set(database_info[key]) - set(danbooru_info[key])\n",
    "                # ignore meta tags\n",
    "                difference_dict[0][key] = [tag for tag in difference_dict[0][key] if not should_ignore_tag(tag)]\n",
    "                difference_dict[1][key] = [tag for tag in difference_dict[1][key] if not should_ignore_tag(tag)]\n",
    "        else:\n",
    "            # update values\n",
    "            if key == \"file_url\":\n",
    "                # check incoming url is valid\n",
    "                if not danbooru_info[key]:\n",
    "                    continue\n",
    "            if danbooru_info[key] != database_info[key]:\n",
    "                difference_dict[0][key] = database_info[key]\n",
    "                # we only need to update the database from danbooru\n",
    "    return difference_dict\n",
    "\n",
    "from functools import cache\n",
    "@cache\n",
    "def should_ignore_tag(tag_id):\n",
    "    \"\"\"\n",
    "    Check if a tag should be ignored\n",
    "    \"\"\"\n",
    "    tag = Tag.get_by_id(tag_id)\n",
    "    if tag is None:\n",
    "        return False\n",
    "    return \"bad\" in tag.name and \"id\" in tag.name # ignore bad_*_id tags\n",
    "import threading\n",
    "from queue import Queue, Empty\n",
    "queue = Queue()\n",
    "event = threading.Event()\n",
    "pbar = None\n",
    "def threaded_executor():\n",
    "    global pbar\n",
    "    while True:\n",
    "        try:\n",
    "            task = queue.get(timeout=0.1)\n",
    "            task()\n",
    "            logging.info(\"Transaction complete\")\n",
    "            if pbar is not None:\n",
    "                pbar.update(1)\n",
    "        except Empty:\n",
    "            if event.is_set():\n",
    "                logging.info(\"Thread exiting\")\n",
    "                break\n",
    "            else:\n",
    "                logging.debug(\"Thread sleeping\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            if isinstance(e, KeyboardInterrupt):\n",
    "                logging.info(\"Thread exiting\")\n",
    "                break\n",
    "            logging.exception(\"Error in thread: {}\".format(e))\n",
    "            continue\n",
    "# if thread is already defined, don't create a new one\n",
    "if \"thread\" not in globals():\n",
    "    thread = threading.Thread(target=threaded_executor)\n",
    "    thread.start()\n",
    "\n",
    "def refresh_thread_and_event():\n",
    "    \"\"\"\n",
    "    Refresh the thread and event\n",
    "    \"\"\"\n",
    "    global thread, event\n",
    "    event.set()\n",
    "    thread.join()\n",
    "    event.clear()\n",
    "    thread = threading.Thread(target=threaded_executor)\n",
    "    thread.start()\n",
    "\n",
    "rate_limit_event = threading.Event()\n",
    "previous_time_sleeped = 2\n",
    "def handle_rate_limit():\n",
    "    \"\"\"\n",
    "    Handle rate limit\n",
    "    \"\"\"\n",
    "    global rate_limit_event\n",
    "    if not rate_limit_event.is_set():\n",
    "        return\n",
    "    previous_time_sleeped *= 2\n",
    "    logging.info(f\"Rate limit reached, sleeping for {previous_time_sleeped} seconds\")\n",
    "    time.sleep(previous_time_sleeped)\n",
    "    rate_limit_event.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_pair(tag_ids_before, tag_ids_after):\n",
    "    \"\"\"\n",
    "    Yields tag pairs which has 1-difference\n",
    "    \"\"\"\n",
    "    for tag_id in tag_ids_before:\n",
    "        if tag_id - 1 in tag_ids_after:\n",
    "            yield tag_id, tag_id - 1\n",
    "        elif tag_id + 1 in tag_ids_after:\n",
    "            yield tag_id, tag_id + 1\n",
    "\n",
    "def patch_differences(id, before, after, submit=True):\n",
    "    \"\"\"\n",
    "    Patch the differences between before and after\n",
    "    \"\"\"\n",
    "    post_by_id = Post.get_by_id(id)\n",
    "    if post_by_id is None:\n",
    "        logging.warning(f\"Post {id} does not exist, patch failed\")\n",
    "        return\n",
    "    for key in before:\n",
    "        if \"tag_list\" in key:\n",
    "            # update tags\n",
    "            tags_list: List[Tag] = getattr(post_by_id, key)\n",
    "            for tag_id_before, tag_id_after in get_tags_pair(before[key], after[key]):\n",
    "                tag_before = Tag.get_by_id(tag_id_before) if isinstance(tag_id_before, int) else create_tag(tag_id_before, key.split(\"_\")[2])\n",
    "                tag_after = Tag.get_by_id(tag_id_after) if isinstance(tag_id_after, int) else create_tag(tag_id_after, key.split(\"_\")[2])\n",
    "                if tag_before is None or tag_after is None:\n",
    "                    logging.warning(f\"Tag {tag_id_before} or {tag_id_after} does not exist, patch failed for post {id}\")\n",
    "                    continue\n",
    "                tags_list.remove(tag_before)\n",
    "                tags_list.append(tag_after)\n",
    "                logging.info(f\"Tag {tag_id_before} replaced by {tag_id_after} for post {id}\")\n",
    "        else:\n",
    "            # update values\n",
    "            setattr(post_by_id, key, after[key])\n",
    "            logging.info(f\"Value {key} updated for post {id}\")\n",
    "    # send transaction to queue\n",
    "    if submit:\n",
    "        queue.put(lambda: post_by_id.save() and patched_posts.set(id))\n",
    "        logging.info(f\"Transaction saved for post {id}, queue size: {queue.qsize()}\")\n",
    "    else:\n",
    "        logging.info(f\"Transaction not saved for post {id}, cached for further use\")\n",
    "def patch_differences_auto(id, submit=True, retry_count=5):\n",
    "    \"\"\"\n",
    "    Automatically patch the differences between before and after\n",
    "    \"\"\"\n",
    "    # if id is tuple, unpack it\n",
    "    if isinstance(id, tuple):\n",
    "        id = id[0]\n",
    "    logging.info(f\"Checking post {id}\")\n",
    "    handle_rate_limit()\n",
    "    for _ in range(retry_count):\n",
    "        try:\n",
    "            difference_dict = difference_database.get(id)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # check 429 error\n",
    "            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 429:\n",
    "                rate_limit_event.set()\n",
    "            logging.exception(f\"Error in post {id}: {e}\")\n",
    "            continue\n",
    "    if difference_dict is None:\n",
    "        logging.warning(f\"Post {id} does not exist, patch failed\")\n",
    "        return\n",
    "    elif len(difference_dict[0]) == 0:\n",
    "        logging.info(f\"Post {id} is up to date\")\n",
    "        return\n",
    "    if submit:\n",
    "        patch_differences(id, difference_dict[1], difference_dict[0], submit=submit)\n",
    "    global pbar\n",
    "    if pbar is not None:\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def patch_differences_auto_multi(ids, threads=4, submit=True, retry_count=5):\n",
    "    \"\"\"\n",
    "    Automatically patch the differences between before and after\n",
    "    \"\"\"\n",
    "    refresh_thread_and_event()\n",
    "    with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        global pbar\n",
    "        pbar = tqdm(total=len(ids))\n",
    "        submit_pbar = tqdm(ids)\n",
    "        for id in submit_pbar:\n",
    "            if patched_posts.get(id):\n",
    "                logging.debug(f\"Post {id} already patched, skipping\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            elif not submit and difference_database.contains(id):\n",
    "                logging.debug(f\"Post {id} already cached, skipping\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            executor.submit(partial(patch_differences_auto, id, submit=submit))\n",
    "        event.set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found finished transactions: {len(patched_posts.cache)}\")\n",
    "print(f\"Found cached differences: {len(difference_database.cache)}\")\n",
    "# lazy iterator for peewee\n",
    "all_post_ids = (Post.select(Post.id).tuples())\n",
    "# debug with 100 posts\n",
    "#all_post_ids = (Post.select(Post.id).tuples().limit(100))\n",
    "print(\"Checking {} posts\".format(len(all_post_ids)))\n",
    "patch_differences_auto_multi(all_post_ids, threads=5, submit=False) # sqlite database modification is not thread safe, so we need to submit them one by one, run the script later to submit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save the database\n",
    "logging.info(\"Saving database\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
